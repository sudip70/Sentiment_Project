{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# 1. Text Length\n",
    "data['text_length'] = data['lemmatized_text'].apply(len)\n",
    "\n",
    "# 2. Word Count\n",
    "data['word_count'] = data['lemmatized_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 3. Stopword Count\n",
    "data['stopword_count'] = data['lemmatized_text'].apply(lambda x: sum(1 for word in x.split() if word in stop_words))\n",
    "\n",
    "# 4. Average Word Length\n",
    "data['avg_word_length'] = data['lemmatized_text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data['lemmatized_text']\n",
    "X_features = data[['text_length', 'word_count', 'stopword_count', 'avg_word_length']]\n",
    "y = data['target']  # 'target' column has sentiment labels (1 for positive, 0 for negative)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_features_scaled = scaler.fit_transform(X_features)\n",
    "# Split the dataset\n",
    "X_train, X_test, X_features_train, X_features_test, y_train, y_test = train_test_split(X, X_features_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text data to numerical vectors using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "# Combine TF-IDF features with engineered features\n",
    "X_train_combined = hstack([X_train_tfidf, X_features_train])\n",
    "X_test_combined = hstack([X_test_tfidf, X_features_test])\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test_combined)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, log_reg.predict_proba(X_test_combined)[:, 1]))\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "grid_search.fit(X_train_combined, y_train)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred_best = best_model.predict(X_test_combined)\n",
    "print(\"Best Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
