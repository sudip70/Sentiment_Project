{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  awww thats bummer shoulda got david carr third...   \n",
      "1  upset cant update facebook texting might cry r...   \n",
      "2  dived many times ball managed save rest go bounds   \n",
      "3                   whole body feels itchy like fire   \n",
      "4                           behaving im mad cant see   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  awww thats bummer shoulda got david carr third...  \n",
      "1  upset cant update facebook texting might cry r...  \n",
      "2    dived many time ball managed save rest go bound  \n",
      "3                    whole body feel itchy like fire  \n",
      "4                           behaving im mad cant see  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Fill any missing values in 'cleaned_text' with an empty string\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('')\n",
    "\n",
    "# Initialize the lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for tokenization and lemmatization\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Lemmatize each token\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "    \n",
    "\n",
    "# Apply the function to the 'cleaned_text' column\n",
    "data['lemmatized_text'] = data['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Save the processed data to a new CSV file to avoid reprocessing next time\n",
    "data.to_csv(\"lemmatized_data.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(data[['cleaned_text', 'lemmatized_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where the columns are the same: 964328\n",
      "Number of rows where the columns are different: 635672\n",
      "                                              cleaned_text  \\\n",
      "2        dived many times ball managed save rest go bounds   \n",
      "3                         whole body feels itchy like fire   \n",
      "7        hey long time see yes rains bit bit lol im fin...   \n",
      "11                                          repierced ears   \n",
      "13                    counts idk either never talk anymore   \n",
      "...                                                    ...   \n",
      "1599986                                      much ads blog   \n",
      "1599988  ha good job thats right gotta throw bigrun tag...   \n",
      "1599991  mmmm sounds absolutely perfect schedule full w...   \n",
      "1599996            thewdbcom cool hear old walt interviews   \n",
      "1599997                    ready mojo makeover ask details   \n",
      "\n",
      "                                           lemmatized_text  \n",
      "2          dived many time ball managed save rest go bound  \n",
      "3                          whole body feel itchy like fire  \n",
      "7        hey long time see yes rain bit bit lol im fine...  \n",
      "11                                           repierced ear  \n",
      "13                     count idk either never talk anymore  \n",
      "...                                                    ...  \n",
      "1599986                                       much ad blog  \n",
      "1599988  ha good job thats right got ta throw bigrun ta...  \n",
      "1599991  mmmm sound absolutely perfect schedule full wo...  \n",
      "1599996             thewdbcom cool hear old walt interview  \n",
      "1599997                     ready mojo makeover ask detail  \n",
      "\n",
      "[635672 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns \"cleaned_text\" and \"lemmatized_text\" are the same\n",
    "data['is_same'] = data['cleaned_text'] == data['lemmatized_text']\n",
    "\n",
    "# Display rows where the columns are not the same\n",
    "different_rows = data[data['is_same'] == False]\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of rows where the columns are the same: {data['is_same'].sum()}\")\n",
    "print(f\"Number of rows where the columns are different: {len(different_rows)}\")\n",
    "\n",
    "# Optionally display the rows where they are different\n",
    "print(different_rows[['cleaned_text', 'lemmatized_text']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
