{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\Acer\\Labs\\DSMM_Term_02\\BhavikG_App1034\\Project\\dev-br\\dev\\cleaned_data.csv\")\n",
    "\n",
    "# Fill any missing values in 'cleaned_text' with an empty string\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('')\n",
    "\n",
    "# Initialize the lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for tokenization and lemmatization\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Lemmatize each token\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return ' '.join(lemmatized_text)\n",
    "    \n",
    "\n",
    "# Apply the function to the 'cleaned_text' column\n",
    "data['lemmatized_text'] = data['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Save the processed data to a new CSV file to avoid reprocessing next time\n",
    "data.to_csv(r\"C:\\Users\\Acer\\Labs\\DSMM_Term_02\\BhavikG_App1034\\Project\\dev-br\\dev\\lemmatized_data.csv\", index=False)\n",
    "\n",
    "# Display the results\n",
    "print(data[['cleaned_text', 'lemmatized_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where the columns are the same: 964328\n",
      "Number of rows where the columns are different: 635672\n",
      "                                              cleaned_text  \\\n",
      "2        dived many times ball managed save rest go bounds   \n",
      "3                         whole body feels itchy like fire   \n",
      "7        hey long time see yes rains bit bit lol im fin...   \n",
      "11                                          repierced ears   \n",
      "13                    counts idk either never talk anymore   \n",
      "...                                                    ...   \n",
      "1599986                                      much ads blog   \n",
      "1599988  ha good job thats right gotta throw bigrun tag...   \n",
      "1599991  mmmm sounds absolutely perfect schedule full w...   \n",
      "1599996            thewdbcom cool hear old walt interviews   \n",
      "1599997                    ready mojo makeover ask details   \n",
      "\n",
      "                                           lemmatized_text  \n",
      "2          dived many time ball managed save rest go bound  \n",
      "3                          whole body feel itchy like fire  \n",
      "7        hey long time see yes rain bit bit lol im fine...  \n",
      "11                                           repierced ear  \n",
      "13                     count idk either never talk anymore  \n",
      "...                                                    ...  \n",
      "1599986                                       much ad blog  \n",
      "1599988  ha good job thats right got ta throw bigrun ta...  \n",
      "1599991  mmmm sound absolutely perfect schedule full wo...  \n",
      "1599996             thewdbcom cool hear old walt interview  \n",
      "1599997                     ready mojo makeover ask detail  \n",
      "\n",
      "[635672 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if the columns \"cleaned_text\" and \"lemmatized_text\" are the same\n",
    "data['is_same'] = data['cleaned_text'] == data['lemmatized_text']\n",
    "\n",
    "# Display rows where the columns are not the same\n",
    "different_rows = data[data['is_same'] == False]\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of rows where the columns are the same: {data['is_same'].sum()}\")\n",
    "print(f\"Number of rows where the columns are different: {len(different_rows)}\")\n",
    "\n",
    "# Optionally display the rows where they are different\n",
    "print(different_rows[['cleaned_text', 'lemmatized_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77440625\n",
      "Confusion Matrix:\n",
      " [[120342  39658]\n",
      " [ 32532 127468]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77    160000\n",
      "           1       0.76      0.80      0.78    160000\n",
      "\n",
      "    accuracy                           0.77    320000\n",
      "   macro avg       0.77      0.77      0.77    320000\n",
      "weighted avg       0.77      0.77      0.77    320000\n",
      "\n",
      "AUC Score: 0.8542522505664063\n",
      "Best Parameters: {'C': 10}\n",
      "Best Model Accuracy: 0.774175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "# Load the lemmatized dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\Acer\\Labs\\DSMM_Term_02\\BhavikG_App1034\\Project\\dev-br\\dev\\lemmatized_data.csv\")\n",
    "\n",
    "# Ensure there are no NaN values in the 'lemmatized_text' column\n",
    "data['lemmatized_text'] = data['lemmatized_text'].fillna('')\n",
    "\n",
    "# Prepare features and target\n",
    "X = data['lemmatized_text']\n",
    "y = data['target']  # 'target' column has sentiment labels (1 for positive, 0 for negative)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text data to numerical vectors using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, log_reg.predict_proba(X_test_tfidf)[:, 1]))\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred_best = best_model.predict(X_test_tfidf)\n",
    "print(\"Best Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 41.0/60.6 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.6/60.6 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/24.0 MB 6.4 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.5/24.0 MB 5.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.8/24.0 MB 6.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.1/24.0 MB 6.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.6/24.0 MB 7.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.2/24.0 MB 8.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.8/24.0 MB 9.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.0 MB 9.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 3.9/24.0 MB 9.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.4/24.0 MB 9.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.1/24.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 11.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.0/24.0 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.3/24.0 MB 12.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.5/24.0 MB 13.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.8/24.0 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.6/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.4/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.0/24.0 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.6/24.0 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.5/24.0 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.8/24.0 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.1/24.0 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.2/24.0 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.3/24.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 20.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 22.9/24.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.0/45.9 MB 63.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 3.9/45.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 6.0/45.9 MB 47.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 7.9/45.9 MB 45.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.7/45.9 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 13.0/45.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 15.0/45.9 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 17.0/45.9 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 18.6/45.9 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.8/45.9 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 22.5/45.9 MB 43.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 24.0/45.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 26.2/45.9 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.8/45.9 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.8/45.9 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.8/45.9 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 35.8/45.9 MB 50.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.7/45.9 MB 46.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.6/45.9 MB 46.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 42.5/45.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.1/45.9 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.4/61.4 kB ? eta 0:00:00\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.0\n",
      "    Uninstalling scipy-1.14.0:\n",
      "      Successfully uninstalled scipy-1.14.0\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71345625\n",
      "Confusion Matrix:\n",
      " [[113095  46905]\n",
      " [ 44789 115211]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71    160000\n",
      "           1       0.71      0.72      0.72    160000\n",
      "\n",
      "    accuracy                           0.71    320000\n",
      "   macro avg       0.71      0.71      0.71    320000\n",
      "weighted avg       0.71      0.71      0.71    320000\n",
      "\n",
      "AUC Score: 0.786049748359375\n",
      "Best Parameters: {'C': 10}\n",
      "Best Model Accuracy: 0.774175\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example corpus\n",
    "corpus = data['lemmatized_text'].tolist()  # Assuming `lemmatized_text` is preprocessed\n",
    "\n",
    "# Target labels\n",
    "y = data['target']  # Sentiment labels\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "w2v_model = Word2Vec(sentences=[doc.split() for doc in corpus], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate TF-IDF scores\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "tfidf_vocab = tfidf.vocabulary_  # Maps words to their index in TF-IDF\n",
    "\n",
    "# Create a function to compute TF-IDF weighted Word2Vec vectors for each document\n",
    "def tfidf_weighted_avg(doc):\n",
    "    words = doc.split()\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        if word in w2v_model.wv.key_to_index:  # Check if word exists in Word2Vec model\n",
    "            tfidf_weight = tfidf.idf_[tfidf_vocab.get(word, 0)]  # Get TF-IDF weight\n",
    "            word_vec = w2v_model.wv[word] * tfidf_weight  # Weight Word2Vec vector by TF-IDF\n",
    "            word_vecs.append(word_vec)\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# Apply the function to generate TF-IDF weighted Word2Vec vectors for each document\n",
    "X = np.array([tfidf_weighted_avg(doc) for doc in corpus])\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, log_reg.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred_best = best_model.predict(X_test_tfidf)\n",
    "print(\"Best Model Accuracy:\", accuracy_score(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
