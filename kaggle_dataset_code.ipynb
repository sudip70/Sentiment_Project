{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a7dc57-8d8c-4fbc-b277-d34026fb2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "     ---------------------------------------- 0.0/82.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.7/82.7 kB 4.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (4.67.0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.2.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\amrit raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.2/78.2 kB 4.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (pyproject.toml): started\n",
      "  Building wheel for kaggle (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105797 sha256=7d0edce9f705f0eb902e08622a40f0a80892719efd14f070f5cb94f4de2030b7\n",
      "  Stored in directory: c:\\users\\amrit raj\\appdata\\local\\pip\\cache\\wheels\\ff\\55\\fb\\b27a466be754d2a06ffe0e37b248d844f090a63b51becea85d\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c29fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d30ea1-da2a-43b4-a840-b5c739cbd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.9M [00:00<?, ?B/s]\n",
      "  1%|1         | 1.00M/80.9M [00:00<00:10, 8.32MB/s]\n",
      "  4%|3         | 3.00M/80.9M [00:00<00:05, 13.8MB/s]\n",
      "  6%|6         | 5.00M/80.9M [00:00<00:04, 16.6MB/s]\n",
      "  9%|8         | 7.00M/80.9M [00:00<00:04, 17.1MB/s]\n",
      " 11%|#1        | 9.00M/80.9M [00:00<00:04, 17.9MB/s]\n",
      " 14%|#3        | 11.0M/80.9M [00:00<00:03, 18.7MB/s]\n",
      " 16%|#6        | 13.0M/80.9M [00:00<00:04, 16.5MB/s]\n",
      " 19%|#8        | 15.0M/80.9M [00:00<00:03, 17.5MB/s]\n",
      " 21%|##1       | 17.0M/80.9M [00:01<00:03, 17.7MB/s]\n",
      " 23%|##3       | 19.0M/80.9M [00:01<00:03, 17.5MB/s]\n",
      " 26%|##5       | 21.0M/80.9M [00:01<00:03, 18.0MB/s]\n",
      " 28%|##8       | 23.0M/80.9M [00:01<00:03, 18.2MB/s]\n",
      " 31%|###       | 25.0M/80.9M [00:01<00:03, 18.7MB/s]\n",
      " 33%|###3      | 27.0M/80.9M [00:01<00:03, 18.0MB/s]\n",
      " 36%|###5      | 29.0M/80.9M [00:01<00:02, 18.8MB/s]\n",
      " 38%|###8      | 31.0M/80.9M [00:01<00:02, 19.0MB/s]\n",
      " 41%|####      | 33.0M/80.9M [00:01<00:02, 19.0MB/s]\n",
      " 44%|####4     | 36.0M/80.9M [00:02<00:02, 20.4MB/s]\n",
      " 47%|####6     | 38.0M/80.9M [00:02<00:02, 20.4MB/s]\n",
      " 51%|#####     | 41.0M/80.9M [00:02<00:02, 20.5MB/s]\n",
      " 54%|#####4    | 44.0M/80.9M [00:02<00:01, 20.6MB/s]\n",
      " 58%|#####8    | 47.0M/80.9M [00:02<00:01, 20.6MB/s]\n",
      " 61%|######    | 49.0M/80.9M [00:02<00:01, 20.0MB/s]\n",
      " 64%|######4   | 52.0M/80.9M [00:02<00:01, 20.7MB/s]\n",
      " 68%|######7   | 55.0M/80.9M [00:03<00:01, 21.0MB/s]\n",
      " 70%|#######   | 57.0M/80.9M [00:03<00:01, 20.2MB/s]\n",
      " 74%|#######4  | 60.0M/80.9M [00:03<00:01, 21.6MB/s]\n",
      " 78%|#######7  | 63.0M/80.9M [00:03<00:00, 20.6MB/s]\n",
      " 82%|########1 | 66.0M/80.9M [00:03<00:00, 21.1MB/s]\n",
      " 85%|########5 | 69.0M/80.9M [00:03<00:00, 19.8MB/s]\n",
      " 88%|########7 | 71.0M/80.9M [00:03<00:00, 19.9MB/s]\n",
      " 91%|#########1| 74.0M/80.9M [00:04<00:00, 20.9MB/s]\n",
      " 95%|#########5| 77.0M/80.9M [00:04<00:00, 20.6MB/s]\n",
      " 99%|#########8| 80.0M/80.9M [00:04<00:00, 21.0MB/s]\n",
      "100%|##########| 80.9M/80.9M [00:04<00:00, 19.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "Downloading sentiment140.zip to C:\\\\Users\\\\Amrit Raj\\\\Downloads\\\\training_data\n",
      "\n",
      "Dataset downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"C:\\\\Users\\\\Amrit Raj\\\\.kaggle\"  \n",
    "\n",
    "!kaggle datasets download -d kazanova/sentiment140 -p \"C:\\\\Users\\\\Amrit Raj\\\\Downloads\\\\training_data\"\n",
    "\n",
    "\n",
    "zip_path = \"C:\\\\Users\\\\Amrit Raj\\\\Downloads\\\\training_data\\\\sentiment140.zip\" \n",
    "extract_path = \"C:\\\\Users\\\\Amrit Raj\\\\Downloads\\\\training_data\"  \n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The zip file was not found. Please check the download path.\")\n",
    "import pandas as pd\n",
    "file_path = \"C:\\\\Users\\\\Amrit Raj\\\\Downloads\\\\training_data\\\\training.1600000.processed.noemoticon.csv\" \n",
    "df = pd.read_csv(file_path, encoding='latin1', names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f41d18-1589-4263-b6d4-b8b6874a9dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Text data cleaned.\n",
      "Target values replaced: 4 -> 1 for positive sentiment.\n",
      "Total Tweets: 1600000\n",
      "target\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n",
      "Cleaned data saved to C:/Users/lenovo/Documents/projects/training_data/cleaned_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#Downloading stopwords for data cleaning\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "class data_cleaning:\n",
    "    def __init__(self, file_path):\n",
    "        #Initialize with the path to the CSV file\n",
    "        self.file_path = file_path\n",
    "        self.df_data = None\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        \n",
    "    def load_data(self):\n",
    "        #Loading data from CSV file\n",
    "        self.df_data = pd.read_csv(self.file_path, encoding='latin-1', \n",
    "                                   names=['target', 'id', 'date', 'flag', 'user', 'text'])\n",
    "        print(\"Data loaded successfully.\")\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        #Clean the text by removing URLs, mentions, special characters, and stopwords\n",
    "        text = re.sub(r'http\\S+', '', text)  #Removes URLs\n",
    "        text = re.sub(r'@\\w+', '', text)     #Removes mentions\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)  #Removes special characters\n",
    "        text = text.lower()  #Converts to lowercase\n",
    "        text = ' '.join(word for word in text.split() if word not in self.stop_word)  #Removes stopwords\n",
    "        return text\n",
    "    \n",
    "    def clean_data(self):\n",
    "        #Applying text cleaning to the dataset\n",
    "        if self.df_data is not None:\n",
    "            self.df_data['cleaned_text'] = self.df_data['text'].apply(self.clean_text)\n",
    "            print(\"Text data cleaned.\")\n",
    "        else:\n",
    "            print(\"Data not loaded yet. Please load the data first.\")\n",
    "    \n",
    "    def replace_target_values(self):\n",
    "        #Replacing target values: 4 -> 1 for positive sentiment\n",
    "        if self.df_data is not None:\n",
    "            self.df_data['target'] = self.df_data['target'].replace(4, 1)\n",
    "            print(\"Target values replaced: 4 -> 1 for positive sentiment.\")\n",
    "        else:\n",
    "            print(\"Data not loaded yet. Please load the data first.\")\n",
    "    \n",
    "    def save_cleaned_data(self, output_file):\n",
    "        #Saving the cleaned data to a new CSV file\n",
    "        if self.df_data is not None:\n",
    "            self.df_data.to_csv(output_file, index=False)\n",
    "            print(f\"Cleaned data saved to {output_file}.\")\n",
    "        else:\n",
    "            print(\"No data to save. Please clean the data first.\")\n",
    "    \n",
    "    def sentiment_distribution(self):\n",
    "        #Checking sentiment distribution\n",
    "        if self.df_data is not None:\n",
    "            print(f\"Total Tweets: {len(self.df_data)}\")\n",
    "            print(self.df_data['target'].value_counts())\n",
    "        else:\n",
    "            print(\"Data not loaded yet. Please load the data first.\")\n",
    "\n",
    "#Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'C:/Users/lenovo/Documents/projects/training_data/train.csv'\n",
    "    output_file = 'C:/Users/lenovo/Documents/projects/training_data/cleaned_data.csv'\n",
    "    \n",
    "    #Calling data_cleaning class\n",
    "    data = data_cleaning(file_path)\n",
    "    \n",
    "    #Loading the dataset\n",
    "    data.load_data()\n",
    "    \n",
    "    #Cleaning the text data\n",
    "    data.clean_data()\n",
    "    \n",
    "    #Replacing target values (4 -> 1)\n",
    "    data.replace_target_values()\n",
    "    \n",
    "    #Checking sentiment dirtibution\n",
    "    data.sentiment_distribution()\n",
    "    \n",
    "    #Saving the cleaned data to a new file\n",
    "    data.save_cleaned_data(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Fill any missing values in 'cleaned_text' with an empty string\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for tokenization and lemmatization\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize each token\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the function to the 'cleaned_text' column\n",
    "data['lemmatized_text'] = data['cleaned_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Display the results\n",
    "print(data[['cleaned_text', 'lemmatized_text']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
